{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the Bunch type is actually a dictionary,\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: [[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n",
      "Targets:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n",
      "Feature Names:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "#The values of this dictionary are just numpy arrays\n",
    "print(\"Data:\", data.data)\n",
    "print(\"Targets: \",data.target)\n",
    "print(\"Feature Names: \", data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now let's split the dataset into training data and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3)\n",
    "N, D = X_train.shape\n",
    "N,D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first import StandardScaler() from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# now let's scale X and Y using StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "#we don't actually need to scale Y this time, because it's a categorical value (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's build our model using the Sequential pytorch model with 1 linear layer with sigmoid activation function\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "#Use BCELoss (binary cross entropy) and Adam optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "#Convert scaled data into torch tensors\n",
    "X_train_scaled_tensor = torch.from_numpy(X_train_scaled.astype(np.float32))\n",
    "X_test_scaled_tensor = torch.from_numpy(X_test_scaled.astype(np.float32))\n",
    "\n",
    "#Convert scaled Y data into torch tensors\n",
    "#Remember that the target data must be converted into an array of dimensions Dx1 before training\n",
    "y_train_scaled_tensor = torch.from_numpy(y_train.astype(np.float32)).reshape(-1,1)\n",
    "y_test_scaled_tensor = torch.from_numpy(y_test.astype(np.float32)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 | Loss: 0.7526\n",
      "Epoch 2/1000 | Loss: 0.7448\n",
      "Epoch 3/1000 | Loss: 0.7371\n",
      "Epoch 4/1000 | Loss: 0.7294\n",
      "Epoch 5/1000 | Loss: 0.7218\n",
      "Epoch 6/1000 | Loss: 0.7144\n",
      "Epoch 7/1000 | Loss: 0.7070\n",
      "Epoch 8/1000 | Loss: 0.6997\n",
      "Epoch 9/1000 | Loss: 0.6924\n",
      "Epoch 10/1000 | Loss: 0.6853\n",
      "Epoch 11/1000 | Loss: 0.6783\n",
      "Epoch 12/1000 | Loss: 0.6713\n",
      "Epoch 13/1000 | Loss: 0.6644\n",
      "Epoch 14/1000 | Loss: 0.6577\n",
      "Epoch 15/1000 | Loss: 0.6510\n",
      "Epoch 16/1000 | Loss: 0.6444\n",
      "Epoch 17/1000 | Loss: 0.6379\n",
      "Epoch 18/1000 | Loss: 0.6315\n",
      "Epoch 19/1000 | Loss: 0.6252\n",
      "Epoch 20/1000 | Loss: 0.6190\n",
      "Epoch 21/1000 | Loss: 0.6129\n",
      "Epoch 22/1000 | Loss: 0.6068\n",
      "Epoch 23/1000 | Loss: 0.6009\n",
      "Epoch 24/1000 | Loss: 0.5950\n",
      "Epoch 25/1000 | Loss: 0.5893\n",
      "Epoch 26/1000 | Loss: 0.5836\n",
      "Epoch 27/1000 | Loss: 0.5781\n",
      "Epoch 28/1000 | Loss: 0.5726\n",
      "Epoch 29/1000 | Loss: 0.5672\n",
      "Epoch 30/1000 | Loss: 0.5619\n",
      "Epoch 31/1000 | Loss: 0.5567\n",
      "Epoch 32/1000 | Loss: 0.5516\n",
      "Epoch 33/1000 | Loss: 0.5465\n",
      "Epoch 34/1000 | Loss: 0.5416\n",
      "Epoch 35/1000 | Loss: 0.5367\n",
      "Epoch 36/1000 | Loss: 0.5319\n",
      "Epoch 37/1000 | Loss: 0.5272\n",
      "Epoch 38/1000 | Loss: 0.5226\n",
      "Epoch 39/1000 | Loss: 0.5180\n",
      "Epoch 40/1000 | Loss: 0.5136\n",
      "Epoch 41/1000 | Loss: 0.5092\n",
      "Epoch 42/1000 | Loss: 0.5049\n",
      "Epoch 43/1000 | Loss: 0.5006\n",
      "Epoch 44/1000 | Loss: 0.4964\n",
      "Epoch 45/1000 | Loss: 0.4923\n",
      "Epoch 46/1000 | Loss: 0.4883\n",
      "Epoch 47/1000 | Loss: 0.4844\n",
      "Epoch 48/1000 | Loss: 0.4805\n",
      "Epoch 49/1000 | Loss: 0.4766\n",
      "Epoch 50/1000 | Loss: 0.4729\n",
      "Epoch 51/1000 | Loss: 0.4692\n",
      "Epoch 52/1000 | Loss: 0.4655\n",
      "Epoch 53/1000 | Loss: 0.4620\n",
      "Epoch 54/1000 | Loss: 0.4585\n",
      "Epoch 55/1000 | Loss: 0.4550\n",
      "Epoch 56/1000 | Loss: 0.4516\n",
      "Epoch 57/1000 | Loss: 0.4483\n",
      "Epoch 58/1000 | Loss: 0.4450\n",
      "Epoch 59/1000 | Loss: 0.4418\n",
      "Epoch 60/1000 | Loss: 0.4386\n",
      "Epoch 61/1000 | Loss: 0.4355\n",
      "Epoch 62/1000 | Loss: 0.4324\n",
      "Epoch 63/1000 | Loss: 0.4294\n",
      "Epoch 64/1000 | Loss: 0.4264\n",
      "Epoch 65/1000 | Loss: 0.4234\n",
      "Epoch 66/1000 | Loss: 0.4206\n",
      "Epoch 67/1000 | Loss: 0.4177\n",
      "Epoch 68/1000 | Loss: 0.4149\n",
      "Epoch 69/1000 | Loss: 0.4122\n",
      "Epoch 70/1000 | Loss: 0.4095\n",
      "Epoch 71/1000 | Loss: 0.4068\n",
      "Epoch 72/1000 | Loss: 0.4042\n",
      "Epoch 73/1000 | Loss: 0.4016\n",
      "Epoch 74/1000 | Loss: 0.3991\n",
      "Epoch 75/1000 | Loss: 0.3966\n",
      "Epoch 76/1000 | Loss: 0.3941\n",
      "Epoch 77/1000 | Loss: 0.3917\n",
      "Epoch 78/1000 | Loss: 0.3893\n",
      "Epoch 79/1000 | Loss: 0.3869\n",
      "Epoch 80/1000 | Loss: 0.3846\n",
      "Epoch 81/1000 | Loss: 0.3823\n",
      "Epoch 82/1000 | Loss: 0.3800\n",
      "Epoch 83/1000 | Loss: 0.3778\n",
      "Epoch 84/1000 | Loss: 0.3756\n",
      "Epoch 85/1000 | Loss: 0.3735\n",
      "Epoch 86/1000 | Loss: 0.3713\n",
      "Epoch 87/1000 | Loss: 0.3692\n",
      "Epoch 88/1000 | Loss: 0.3671\n",
      "Epoch 89/1000 | Loss: 0.3651\n",
      "Epoch 90/1000 | Loss: 0.3631\n",
      "Epoch 91/1000 | Loss: 0.3611\n",
      "Epoch 92/1000 | Loss: 0.3591\n",
      "Epoch 93/1000 | Loss: 0.3572\n",
      "Epoch 94/1000 | Loss: 0.3553\n",
      "Epoch 95/1000 | Loss: 0.3534\n",
      "Epoch 96/1000 | Loss: 0.3515\n",
      "Epoch 97/1000 | Loss: 0.3497\n",
      "Epoch 98/1000 | Loss: 0.3478\n",
      "Epoch 99/1000 | Loss: 0.3461\n",
      "Epoch 100/1000 | Loss: 0.3443\n",
      "Epoch 101/1000 | Loss: 0.3425\n",
      "Epoch 102/1000 | Loss: 0.3408\n",
      "Epoch 103/1000 | Loss: 0.3391\n",
      "Epoch 104/1000 | Loss: 0.3374\n",
      "Epoch 105/1000 | Loss: 0.3357\n",
      "Epoch 106/1000 | Loss: 0.3341\n",
      "Epoch 107/1000 | Loss: 0.3325\n",
      "Epoch 108/1000 | Loss: 0.3309\n",
      "Epoch 109/1000 | Loss: 0.3293\n",
      "Epoch 110/1000 | Loss: 0.3277\n",
      "Epoch 111/1000 | Loss: 0.3262\n",
      "Epoch 112/1000 | Loss: 0.3246\n",
      "Epoch 113/1000 | Loss: 0.3231\n",
      "Epoch 114/1000 | Loss: 0.3216\n",
      "Epoch 115/1000 | Loss: 0.3201\n",
      "Epoch 116/1000 | Loss: 0.3187\n",
      "Epoch 117/1000 | Loss: 0.3172\n",
      "Epoch 118/1000 | Loss: 0.3158\n",
      "Epoch 119/1000 | Loss: 0.3144\n",
      "Epoch 120/1000 | Loss: 0.3130\n",
      "Epoch 121/1000 | Loss: 0.3116\n",
      "Epoch 122/1000 | Loss: 0.3102\n",
      "Epoch 123/1000 | Loss: 0.3089\n",
      "Epoch 124/1000 | Loss: 0.3075\n",
      "Epoch 125/1000 | Loss: 0.3062\n",
      "Epoch 126/1000 | Loss: 0.3049\n",
      "Epoch 127/1000 | Loss: 0.3036\n",
      "Epoch 128/1000 | Loss: 0.3023\n",
      "Epoch 129/1000 | Loss: 0.3010\n",
      "Epoch 130/1000 | Loss: 0.2997\n",
      "Epoch 131/1000 | Loss: 0.2985\n",
      "Epoch 132/1000 | Loss: 0.2973\n",
      "Epoch 133/1000 | Loss: 0.2960\n",
      "Epoch 134/1000 | Loss: 0.2948\n",
      "Epoch 135/1000 | Loss: 0.2936\n",
      "Epoch 136/1000 | Loss: 0.2924\n",
      "Epoch 137/1000 | Loss: 0.2913\n",
      "Epoch 138/1000 | Loss: 0.2901\n",
      "Epoch 139/1000 | Loss: 0.2889\n",
      "Epoch 140/1000 | Loss: 0.2878\n",
      "Epoch 141/1000 | Loss: 0.2867\n",
      "Epoch 142/1000 | Loss: 0.2855\n",
      "Epoch 143/1000 | Loss: 0.2844\n",
      "Epoch 144/1000 | Loss: 0.2833\n",
      "Epoch 145/1000 | Loss: 0.2822\n",
      "Epoch 146/1000 | Loss: 0.2812\n",
      "Epoch 147/1000 | Loss: 0.2801\n",
      "Epoch 148/1000 | Loss: 0.2790\n",
      "Epoch 149/1000 | Loss: 0.2780\n",
      "Epoch 150/1000 | Loss: 0.2769\n",
      "Epoch 151/1000 | Loss: 0.2759\n",
      "Epoch 152/1000 | Loss: 0.2749\n",
      "Epoch 153/1000 | Loss: 0.2739\n",
      "Epoch 154/1000 | Loss: 0.2728\n",
      "Epoch 155/1000 | Loss: 0.2719\n",
      "Epoch 156/1000 | Loss: 0.2709\n",
      "Epoch 157/1000 | Loss: 0.2699\n",
      "Epoch 158/1000 | Loss: 0.2689\n",
      "Epoch 159/1000 | Loss: 0.2679\n",
      "Epoch 160/1000 | Loss: 0.2670\n",
      "Epoch 161/1000 | Loss: 0.2660\n",
      "Epoch 162/1000 | Loss: 0.2651\n",
      "Epoch 163/1000 | Loss: 0.2642\n",
      "Epoch 164/1000 | Loss: 0.2632\n",
      "Epoch 165/1000 | Loss: 0.2623\n",
      "Epoch 166/1000 | Loss: 0.2614\n",
      "Epoch 167/1000 | Loss: 0.2605\n",
      "Epoch 168/1000 | Loss: 0.2596\n",
      "Epoch 169/1000 | Loss: 0.2587\n",
      "Epoch 170/1000 | Loss: 0.2579\n",
      "Epoch 171/1000 | Loss: 0.2570\n",
      "Epoch 172/1000 | Loss: 0.2561\n",
      "Epoch 173/1000 | Loss: 0.2553\n",
      "Epoch 174/1000 | Loss: 0.2544\n",
      "Epoch 175/1000 | Loss: 0.2536\n",
      "Epoch 176/1000 | Loss: 0.2527\n",
      "Epoch 177/1000 | Loss: 0.2519\n",
      "Epoch 178/1000 | Loss: 0.2511\n",
      "Epoch 179/1000 | Loss: 0.2503\n",
      "Epoch 180/1000 | Loss: 0.2494\n",
      "Epoch 181/1000 | Loss: 0.2486\n",
      "Epoch 182/1000 | Loss: 0.2478\n",
      "Epoch 183/1000 | Loss: 0.2470\n",
      "Epoch 184/1000 | Loss: 0.2462\n",
      "Epoch 185/1000 | Loss: 0.2455\n",
      "Epoch 186/1000 | Loss: 0.2447\n",
      "Epoch 187/1000 | Loss: 0.2439\n",
      "Epoch 188/1000 | Loss: 0.2432\n",
      "Epoch 189/1000 | Loss: 0.2424\n",
      "Epoch 190/1000 | Loss: 0.2416\n",
      "Epoch 191/1000 | Loss: 0.2409\n",
      "Epoch 192/1000 | Loss: 0.2401\n",
      "Epoch 193/1000 | Loss: 0.2394\n",
      "Epoch 194/1000 | Loss: 0.2387\n",
      "Epoch 195/1000 | Loss: 0.2379\n",
      "Epoch 196/1000 | Loss: 0.2372\n",
      "Epoch 197/1000 | Loss: 0.2365\n",
      "Epoch 198/1000 | Loss: 0.2358\n",
      "Epoch 199/1000 | Loss: 0.2351\n",
      "Epoch 200/1000 | Loss: 0.2344\n",
      "Epoch 201/1000 | Loss: 0.2337\n",
      "Epoch 202/1000 | Loss: 0.2330\n",
      "Epoch 203/1000 | Loss: 0.2323\n",
      "Epoch 204/1000 | Loss: 0.2316\n",
      "Epoch 205/1000 | Loss: 0.2310\n",
      "Epoch 206/1000 | Loss: 0.2303\n",
      "Epoch 207/1000 | Loss: 0.2296\n",
      "Epoch 208/1000 | Loss: 0.2289\n",
      "Epoch 209/1000 | Loss: 0.2283\n",
      "Epoch 210/1000 | Loss: 0.2276\n",
      "Epoch 211/1000 | Loss: 0.2270\n",
      "Epoch 212/1000 | Loss: 0.2263\n",
      "Epoch 213/1000 | Loss: 0.2257\n",
      "Epoch 214/1000 | Loss: 0.2251\n",
      "Epoch 215/1000 | Loss: 0.2244\n",
      "Epoch 216/1000 | Loss: 0.2238\n",
      "Epoch 217/1000 | Loss: 0.2232\n",
      "Epoch 218/1000 | Loss: 0.2225\n",
      "Epoch 219/1000 | Loss: 0.2219\n",
      "Epoch 220/1000 | Loss: 0.2213\n",
      "Epoch 221/1000 | Loss: 0.2207\n",
      "Epoch 222/1000 | Loss: 0.2201\n",
      "Epoch 223/1000 | Loss: 0.2195\n",
      "Epoch 224/1000 | Loss: 0.2189\n",
      "Epoch 225/1000 | Loss: 0.2183\n",
      "Epoch 226/1000 | Loss: 0.2177\n",
      "Epoch 227/1000 | Loss: 0.2171\n",
      "Epoch 228/1000 | Loss: 0.2166\n",
      "Epoch 229/1000 | Loss: 0.2160\n",
      "Epoch 230/1000 | Loss: 0.2154\n",
      "Epoch 231/1000 | Loss: 0.2148\n",
      "Epoch 232/1000 | Loss: 0.2143\n",
      "Epoch 233/1000 | Loss: 0.2137\n",
      "Epoch 234/1000 | Loss: 0.2131\n",
      "Epoch 235/1000 | Loss: 0.2126\n",
      "Epoch 236/1000 | Loss: 0.2120\n",
      "Epoch 237/1000 | Loss: 0.2115\n",
      "Epoch 238/1000 | Loss: 0.2109\n",
      "Epoch 239/1000 | Loss: 0.2104\n",
      "Epoch 240/1000 | Loss: 0.2099\n",
      "Epoch 241/1000 | Loss: 0.2093\n",
      "Epoch 242/1000 | Loss: 0.2088\n",
      "Epoch 243/1000 | Loss: 0.2083\n",
      "Epoch 244/1000 | Loss: 0.2077\n",
      "Epoch 245/1000 | Loss: 0.2072\n",
      "Epoch 246/1000 | Loss: 0.2067\n",
      "Epoch 247/1000 | Loss: 0.2062\n",
      "Epoch 248/1000 | Loss: 0.2057\n",
      "Epoch 249/1000 | Loss: 0.2051\n",
      "Epoch 250/1000 | Loss: 0.2046\n",
      "Epoch 251/1000 | Loss: 0.2041\n",
      "Epoch 252/1000 | Loss: 0.2036\n",
      "Epoch 253/1000 | Loss: 0.2031\n",
      "Epoch 254/1000 | Loss: 0.2026\n",
      "Epoch 255/1000 | Loss: 0.2021\n",
      "Epoch 256/1000 | Loss: 0.2016\n",
      "Epoch 257/1000 | Loss: 0.2012\n",
      "Epoch 258/1000 | Loss: 0.2007\n",
      "Epoch 259/1000 | Loss: 0.2002\n",
      "Epoch 260/1000 | Loss: 0.1997\n",
      "Epoch 261/1000 | Loss: 0.1992\n",
      "Epoch 262/1000 | Loss: 0.1988\n",
      "Epoch 263/1000 | Loss: 0.1983\n",
      "Epoch 264/1000 | Loss: 0.1978\n",
      "Epoch 265/1000 | Loss: 0.1974\n",
      "Epoch 266/1000 | Loss: 0.1969\n",
      "Epoch 267/1000 | Loss: 0.1964\n",
      "Epoch 268/1000 | Loss: 0.1960\n",
      "Epoch 269/1000 | Loss: 0.1955\n",
      "Epoch 270/1000 | Loss: 0.1951\n",
      "Epoch 271/1000 | Loss: 0.1946\n",
      "Epoch 272/1000 | Loss: 0.1942\n",
      "Epoch 273/1000 | Loss: 0.1937\n",
      "Epoch 274/1000 | Loss: 0.1933\n",
      "Epoch 275/1000 | Loss: 0.1928\n",
      "Epoch 276/1000 | Loss: 0.1924\n",
      "Epoch 277/1000 | Loss: 0.1920\n",
      "Epoch 278/1000 | Loss: 0.1915\n",
      "Epoch 279/1000 | Loss: 0.1911\n",
      "Epoch 280/1000 | Loss: 0.1907\n",
      "Epoch 281/1000 | Loss: 0.1902\n",
      "Epoch 282/1000 | Loss: 0.1898\n",
      "Epoch 283/1000 | Loss: 0.1894\n",
      "Epoch 284/1000 | Loss: 0.1890\n",
      "Epoch 285/1000 | Loss: 0.1886\n",
      "Epoch 286/1000 | Loss: 0.1882\n",
      "Epoch 287/1000 | Loss: 0.1877\n",
      "Epoch 288/1000 | Loss: 0.1873\n",
      "Epoch 289/1000 | Loss: 0.1869\n",
      "Epoch 290/1000 | Loss: 0.1865\n",
      "Epoch 291/1000 | Loss: 0.1861\n",
      "Epoch 292/1000 | Loss: 0.1857\n",
      "Epoch 293/1000 | Loss: 0.1853\n",
      "Epoch 294/1000 | Loss: 0.1849\n",
      "Epoch 295/1000 | Loss: 0.1845\n",
      "Epoch 296/1000 | Loss: 0.1841\n",
      "Epoch 297/1000 | Loss: 0.1837\n",
      "Epoch 298/1000 | Loss: 0.1833\n",
      "Epoch 299/1000 | Loss: 0.1830\n",
      "Epoch 300/1000 | Loss: 0.1826\n",
      "Epoch 301/1000 | Loss: 0.1822\n",
      "Epoch 302/1000 | Loss: 0.1818\n",
      "Epoch 303/1000 | Loss: 0.1814\n",
      "Epoch 304/1000 | Loss: 0.1810\n",
      "Epoch 305/1000 | Loss: 0.1807\n",
      "Epoch 306/1000 | Loss: 0.1803\n",
      "Epoch 307/1000 | Loss: 0.1799\n",
      "Epoch 308/1000 | Loss: 0.1796\n",
      "Epoch 309/1000 | Loss: 0.1792\n",
      "Epoch 310/1000 | Loss: 0.1788\n",
      "Epoch 311/1000 | Loss: 0.1785\n",
      "Epoch 312/1000 | Loss: 0.1781\n",
      "Epoch 313/1000 | Loss: 0.1777\n",
      "Epoch 314/1000 | Loss: 0.1774\n",
      "Epoch 315/1000 | Loss: 0.1770\n",
      "Epoch 316/1000 | Loss: 0.1767\n",
      "Epoch 317/1000 | Loss: 0.1763\n",
      "Epoch 318/1000 | Loss: 0.1760\n",
      "Epoch 319/1000 | Loss: 0.1756\n",
      "Epoch 320/1000 | Loss: 0.1753\n",
      "Epoch 321/1000 | Loss: 0.1749\n",
      "Epoch 322/1000 | Loss: 0.1746\n",
      "Epoch 323/1000 | Loss: 0.1742\n",
      "Epoch 324/1000 | Loss: 0.1739\n",
      "Epoch 325/1000 | Loss: 0.1735\n",
      "Epoch 326/1000 | Loss: 0.1732\n",
      "Epoch 327/1000 | Loss: 0.1729\n",
      "Epoch 328/1000 | Loss: 0.1725\n",
      "Epoch 329/1000 | Loss: 0.1722\n",
      "Epoch 330/1000 | Loss: 0.1719\n",
      "Epoch 331/1000 | Loss: 0.1715\n",
      "Epoch 332/1000 | Loss: 0.1712\n",
      "Epoch 333/1000 | Loss: 0.1709\n",
      "Epoch 334/1000 | Loss: 0.1705\n",
      "Epoch 335/1000 | Loss: 0.1702\n",
      "Epoch 336/1000 | Loss: 0.1699\n",
      "Epoch 337/1000 | Loss: 0.1696\n",
      "Epoch 338/1000 | Loss: 0.1693\n",
      "Epoch 339/1000 | Loss: 0.1689\n",
      "Epoch 340/1000 | Loss: 0.1686\n",
      "Epoch 341/1000 | Loss: 0.1683\n",
      "Epoch 342/1000 | Loss: 0.1680\n",
      "Epoch 343/1000 | Loss: 0.1677\n",
      "Epoch 344/1000 | Loss: 0.1674\n",
      "Epoch 345/1000 | Loss: 0.1671\n",
      "Epoch 346/1000 | Loss: 0.1668\n",
      "Epoch 347/1000 | Loss: 0.1664\n",
      "Epoch 348/1000 | Loss: 0.1661\n",
      "Epoch 349/1000 | Loss: 0.1658\n",
      "Epoch 350/1000 | Loss: 0.1655\n",
      "Epoch 351/1000 | Loss: 0.1652\n",
      "Epoch 352/1000 | Loss: 0.1649\n",
      "Epoch 353/1000 | Loss: 0.1646\n",
      "Epoch 354/1000 | Loss: 0.1643\n",
      "Epoch 355/1000 | Loss: 0.1640\n",
      "Epoch 356/1000 | Loss: 0.1637\n",
      "Epoch 357/1000 | Loss: 0.1635\n",
      "Epoch 358/1000 | Loss: 0.1632\n",
      "Epoch 359/1000 | Loss: 0.1629\n",
      "Epoch 360/1000 | Loss: 0.1626\n",
      "Epoch 361/1000 | Loss: 0.1623\n",
      "Epoch 362/1000 | Loss: 0.1620\n",
      "Epoch 363/1000 | Loss: 0.1617\n",
      "Epoch 364/1000 | Loss: 0.1614\n",
      "Epoch 365/1000 | Loss: 0.1612\n",
      "Epoch 366/1000 | Loss: 0.1609\n",
      "Epoch 367/1000 | Loss: 0.1606\n",
      "Epoch 368/1000 | Loss: 0.1603\n",
      "Epoch 369/1000 | Loss: 0.1600\n",
      "Epoch 370/1000 | Loss: 0.1598\n",
      "Epoch 371/1000 | Loss: 0.1595\n",
      "Epoch 372/1000 | Loss: 0.1592\n",
      "Epoch 373/1000 | Loss: 0.1589\n",
      "Epoch 374/1000 | Loss: 0.1587\n",
      "Epoch 375/1000 | Loss: 0.1584\n",
      "Epoch 376/1000 | Loss: 0.1581\n",
      "Epoch 377/1000 | Loss: 0.1579\n",
      "Epoch 378/1000 | Loss: 0.1576\n",
      "Epoch 379/1000 | Loss: 0.1573\n",
      "Epoch 380/1000 | Loss: 0.1571\n",
      "Epoch 381/1000 | Loss: 0.1568\n",
      "Epoch 382/1000 | Loss: 0.1565\n",
      "Epoch 383/1000 | Loss: 0.1563\n",
      "Epoch 384/1000 | Loss: 0.1560\n",
      "Epoch 385/1000 | Loss: 0.1557\n",
      "Epoch 386/1000 | Loss: 0.1555\n",
      "Epoch 387/1000 | Loss: 0.1552\n",
      "Epoch 388/1000 | Loss: 0.1550\n",
      "Epoch 389/1000 | Loss: 0.1547\n",
      "Epoch 390/1000 | Loss: 0.1545\n",
      "Epoch 391/1000 | Loss: 0.1542\n",
      "Epoch 392/1000 | Loss: 0.1540\n",
      "Epoch 393/1000 | Loss: 0.1537\n",
      "Epoch 394/1000 | Loss: 0.1535\n",
      "Epoch 395/1000 | Loss: 0.1532\n",
      "Epoch 396/1000 | Loss: 0.1530\n",
      "Epoch 397/1000 | Loss: 0.1527\n",
      "Epoch 398/1000 | Loss: 0.1525\n",
      "Epoch 399/1000 | Loss: 0.1522\n",
      "Epoch 400/1000 | Loss: 0.1520\n",
      "Epoch 401/1000 | Loss: 0.1517\n",
      "Epoch 402/1000 | Loss: 0.1515\n",
      "Epoch 403/1000 | Loss: 0.1513\n",
      "Epoch 404/1000 | Loss: 0.1510\n",
      "Epoch 405/1000 | Loss: 0.1508\n",
      "Epoch 406/1000 | Loss: 0.1505\n",
      "Epoch 407/1000 | Loss: 0.1503\n",
      "Epoch 408/1000 | Loss: 0.1501\n",
      "Epoch 409/1000 | Loss: 0.1498\n",
      "Epoch 410/1000 | Loss: 0.1496\n",
      "Epoch 411/1000 | Loss: 0.1494\n",
      "Epoch 412/1000 | Loss: 0.1491\n",
      "Epoch 413/1000 | Loss: 0.1489\n",
      "Epoch 414/1000 | Loss: 0.1487\n",
      "Epoch 415/1000 | Loss: 0.1484\n",
      "Epoch 416/1000 | Loss: 0.1482\n",
      "Epoch 417/1000 | Loss: 0.1480\n",
      "Epoch 418/1000 | Loss: 0.1477\n",
      "Epoch 419/1000 | Loss: 0.1475\n",
      "Epoch 420/1000 | Loss: 0.1473\n",
      "Epoch 421/1000 | Loss: 0.1471\n",
      "Epoch 422/1000 | Loss: 0.1469\n",
      "Epoch 423/1000 | Loss: 0.1466\n",
      "Epoch 424/1000 | Loss: 0.1464\n",
      "Epoch 425/1000 | Loss: 0.1462\n",
      "Epoch 426/1000 | Loss: 0.1460\n",
      "Epoch 427/1000 | Loss: 0.1457\n",
      "Epoch 428/1000 | Loss: 0.1455\n",
      "Epoch 429/1000 | Loss: 0.1453\n",
      "Epoch 430/1000 | Loss: 0.1451\n",
      "Epoch 431/1000 | Loss: 0.1449\n",
      "Epoch 432/1000 | Loss: 0.1447\n",
      "Epoch 433/1000 | Loss: 0.1444\n",
      "Epoch 434/1000 | Loss: 0.1442\n",
      "Epoch 435/1000 | Loss: 0.1440\n",
      "Epoch 436/1000 | Loss: 0.1438\n",
      "Epoch 437/1000 | Loss: 0.1436\n",
      "Epoch 438/1000 | Loss: 0.1434\n",
      "Epoch 439/1000 | Loss: 0.1432\n",
      "Epoch 440/1000 | Loss: 0.1430\n",
      "Epoch 441/1000 | Loss: 0.1428\n",
      "Epoch 442/1000 | Loss: 0.1426\n",
      "Epoch 443/1000 | Loss: 0.1423\n",
      "Epoch 444/1000 | Loss: 0.1421\n",
      "Epoch 445/1000 | Loss: 0.1419\n",
      "Epoch 446/1000 | Loss: 0.1417\n",
      "Epoch 447/1000 | Loss: 0.1415\n",
      "Epoch 448/1000 | Loss: 0.1413\n",
      "Epoch 449/1000 | Loss: 0.1411\n",
      "Epoch 450/1000 | Loss: 0.1409\n",
      "Epoch 451/1000 | Loss: 0.1407\n",
      "Epoch 452/1000 | Loss: 0.1405\n",
      "Epoch 453/1000 | Loss: 0.1403\n",
      "Epoch 454/1000 | Loss: 0.1401\n",
      "Epoch 455/1000 | Loss: 0.1399\n",
      "Epoch 456/1000 | Loss: 0.1397\n",
      "Epoch 457/1000 | Loss: 0.1395\n",
      "Epoch 458/1000 | Loss: 0.1393\n",
      "Epoch 459/1000 | Loss: 0.1391\n",
      "Epoch 460/1000 | Loss: 0.1390\n",
      "Epoch 461/1000 | Loss: 0.1388\n",
      "Epoch 462/1000 | Loss: 0.1386\n",
      "Epoch 463/1000 | Loss: 0.1384\n",
      "Epoch 464/1000 | Loss: 0.1382\n",
      "Epoch 465/1000 | Loss: 0.1380\n",
      "Epoch 466/1000 | Loss: 0.1378\n",
      "Epoch 467/1000 | Loss: 0.1376\n",
      "Epoch 468/1000 | Loss: 0.1374\n",
      "Epoch 469/1000 | Loss: 0.1372\n",
      "Epoch 470/1000 | Loss: 0.1370\n",
      "Epoch 471/1000 | Loss: 0.1369\n",
      "Epoch 472/1000 | Loss: 0.1367\n",
      "Epoch 473/1000 | Loss: 0.1365\n",
      "Epoch 474/1000 | Loss: 0.1363\n",
      "Epoch 475/1000 | Loss: 0.1361\n",
      "Epoch 476/1000 | Loss: 0.1359\n",
      "Epoch 477/1000 | Loss: 0.1358\n",
      "Epoch 478/1000 | Loss: 0.1356\n",
      "Epoch 479/1000 | Loss: 0.1354\n",
      "Epoch 480/1000 | Loss: 0.1352\n",
      "Epoch 481/1000 | Loss: 0.1350\n",
      "Epoch 482/1000 | Loss: 0.1349\n",
      "Epoch 483/1000 | Loss: 0.1347\n",
      "Epoch 484/1000 | Loss: 0.1345\n",
      "Epoch 485/1000 | Loss: 0.1343\n",
      "Epoch 486/1000 | Loss: 0.1341\n",
      "Epoch 487/1000 | Loss: 0.1340\n",
      "Epoch 488/1000 | Loss: 0.1338\n",
      "Epoch 489/1000 | Loss: 0.1336\n",
      "Epoch 490/1000 | Loss: 0.1334\n",
      "Epoch 491/1000 | Loss: 0.1333\n",
      "Epoch 492/1000 | Loss: 0.1331\n",
      "Epoch 493/1000 | Loss: 0.1329\n",
      "Epoch 494/1000 | Loss: 0.1327\n",
      "Epoch 495/1000 | Loss: 0.1326\n",
      "Epoch 496/1000 | Loss: 0.1324\n",
      "Epoch 497/1000 | Loss: 0.1322\n",
      "Epoch 498/1000 | Loss: 0.1321\n",
      "Epoch 499/1000 | Loss: 0.1319\n",
      "Epoch 500/1000 | Loss: 0.1317\n",
      "Epoch 501/1000 | Loss: 0.1316\n",
      "Epoch 502/1000 | Loss: 0.1314\n",
      "Epoch 503/1000 | Loss: 0.1312\n",
      "Epoch 504/1000 | Loss: 0.1310\n",
      "Epoch 505/1000 | Loss: 0.1309\n",
      "Epoch 506/1000 | Loss: 0.1307\n",
      "Epoch 507/1000 | Loss: 0.1306\n",
      "Epoch 508/1000 | Loss: 0.1304\n",
      "Epoch 509/1000 | Loss: 0.1302\n",
      "Epoch 510/1000 | Loss: 0.1301\n",
      "Epoch 511/1000 | Loss: 0.1299\n",
      "Epoch 512/1000 | Loss: 0.1297\n",
      "Epoch 513/1000 | Loss: 0.1296\n",
      "Epoch 514/1000 | Loss: 0.1294\n",
      "Epoch 515/1000 | Loss: 0.1293\n",
      "Epoch 516/1000 | Loss: 0.1291\n",
      "Epoch 517/1000 | Loss: 0.1289\n",
      "Epoch 518/1000 | Loss: 0.1288\n",
      "Epoch 519/1000 | Loss: 0.1286\n",
      "Epoch 520/1000 | Loss: 0.1285\n",
      "Epoch 521/1000 | Loss: 0.1283\n",
      "Epoch 522/1000 | Loss: 0.1281\n",
      "Epoch 523/1000 | Loss: 0.1280\n",
      "Epoch 524/1000 | Loss: 0.1278\n",
      "Epoch 525/1000 | Loss: 0.1277\n",
      "Epoch 526/1000 | Loss: 0.1275\n",
      "Epoch 527/1000 | Loss: 0.1274\n",
      "Epoch 528/1000 | Loss: 0.1272\n",
      "Epoch 529/1000 | Loss: 0.1271\n",
      "Epoch 530/1000 | Loss: 0.1269\n",
      "Epoch 531/1000 | Loss: 0.1267\n",
      "Epoch 532/1000 | Loss: 0.1266\n",
      "Epoch 533/1000 | Loss: 0.1264\n",
      "Epoch 534/1000 | Loss: 0.1263\n",
      "Epoch 535/1000 | Loss: 0.1261\n",
      "Epoch 536/1000 | Loss: 0.1260\n",
      "Epoch 537/1000 | Loss: 0.1258\n",
      "Epoch 538/1000 | Loss: 0.1257\n",
      "Epoch 539/1000 | Loss: 0.1255\n",
      "Epoch 540/1000 | Loss: 0.1254\n",
      "Epoch 541/1000 | Loss: 0.1252\n",
      "Epoch 542/1000 | Loss: 0.1251\n",
      "Epoch 543/1000 | Loss: 0.1249\n",
      "Epoch 544/1000 | Loss: 0.1248\n",
      "Epoch 545/1000 | Loss: 0.1247\n",
      "Epoch 546/1000 | Loss: 0.1245\n",
      "Epoch 547/1000 | Loss: 0.1244\n",
      "Epoch 548/1000 | Loss: 0.1242\n",
      "Epoch 549/1000 | Loss: 0.1241\n",
      "Epoch 550/1000 | Loss: 0.1239\n",
      "Epoch 551/1000 | Loss: 0.1238\n",
      "Epoch 552/1000 | Loss: 0.1236\n",
      "Epoch 553/1000 | Loss: 0.1235\n",
      "Epoch 554/1000 | Loss: 0.1234\n",
      "Epoch 555/1000 | Loss: 0.1232\n",
      "Epoch 556/1000 | Loss: 0.1231\n",
      "Epoch 557/1000 | Loss: 0.1229\n",
      "Epoch 558/1000 | Loss: 0.1228\n",
      "Epoch 559/1000 | Loss: 0.1227\n",
      "Epoch 560/1000 | Loss: 0.1225\n",
      "Epoch 561/1000 | Loss: 0.1224\n",
      "Epoch 562/1000 | Loss: 0.1222\n",
      "Epoch 563/1000 | Loss: 0.1221\n",
      "Epoch 564/1000 | Loss: 0.1220\n",
      "Epoch 565/1000 | Loss: 0.1218\n",
      "Epoch 566/1000 | Loss: 0.1217\n",
      "Epoch 567/1000 | Loss: 0.1215\n",
      "Epoch 568/1000 | Loss: 0.1214\n",
      "Epoch 569/1000 | Loss: 0.1213\n",
      "Epoch 570/1000 | Loss: 0.1211\n",
      "Epoch 571/1000 | Loss: 0.1210\n",
      "Epoch 572/1000 | Loss: 0.1209\n",
      "Epoch 573/1000 | Loss: 0.1207\n",
      "Epoch 574/1000 | Loss: 0.1206\n",
      "Epoch 575/1000 | Loss: 0.1205\n",
      "Epoch 576/1000 | Loss: 0.1203\n",
      "Epoch 577/1000 | Loss: 0.1202\n",
      "Epoch 578/1000 | Loss: 0.1201\n",
      "Epoch 579/1000 | Loss: 0.1199\n",
      "Epoch 580/1000 | Loss: 0.1198\n",
      "Epoch 581/1000 | Loss: 0.1197\n",
      "Epoch 582/1000 | Loss: 0.1195\n",
      "Epoch 583/1000 | Loss: 0.1194\n",
      "Epoch 584/1000 | Loss: 0.1193\n",
      "Epoch 585/1000 | Loss: 0.1191\n",
      "Epoch 586/1000 | Loss: 0.1190\n",
      "Epoch 587/1000 | Loss: 0.1189\n",
      "Epoch 588/1000 | Loss: 0.1188\n",
      "Epoch 589/1000 | Loss: 0.1186\n",
      "Epoch 590/1000 | Loss: 0.1185\n",
      "Epoch 591/1000 | Loss: 0.1184\n",
      "Epoch 592/1000 | Loss: 0.1182\n",
      "Epoch 593/1000 | Loss: 0.1181\n",
      "Epoch 594/1000 | Loss: 0.1180\n",
      "Epoch 595/1000 | Loss: 0.1179\n",
      "Epoch 596/1000 | Loss: 0.1177\n",
      "Epoch 597/1000 | Loss: 0.1176\n",
      "Epoch 598/1000 | Loss: 0.1175\n",
      "Epoch 599/1000 | Loss: 0.1174\n",
      "Epoch 600/1000 | Loss: 0.1172\n",
      "Epoch 601/1000 | Loss: 0.1171\n",
      "Epoch 602/1000 | Loss: 0.1170\n",
      "Epoch 603/1000 | Loss: 0.1169\n",
      "Epoch 604/1000 | Loss: 0.1167\n",
      "Epoch 605/1000 | Loss: 0.1166\n",
      "Epoch 606/1000 | Loss: 0.1165\n",
      "Epoch 607/1000 | Loss: 0.1164\n",
      "Epoch 608/1000 | Loss: 0.1163\n",
      "Epoch 609/1000 | Loss: 0.1161\n",
      "Epoch 610/1000 | Loss: 0.1160\n",
      "Epoch 611/1000 | Loss: 0.1159\n",
      "Epoch 612/1000 | Loss: 0.1158\n",
      "Epoch 613/1000 | Loss: 0.1156\n",
      "Epoch 614/1000 | Loss: 0.1155\n",
      "Epoch 615/1000 | Loss: 0.1154\n",
      "Epoch 616/1000 | Loss: 0.1153\n",
      "Epoch 617/1000 | Loss: 0.1152\n",
      "Epoch 618/1000 | Loss: 0.1151\n",
      "Epoch 619/1000 | Loss: 0.1149\n",
      "Epoch 620/1000 | Loss: 0.1148\n",
      "Epoch 621/1000 | Loss: 0.1147\n",
      "Epoch 622/1000 | Loss: 0.1146\n",
      "Epoch 623/1000 | Loss: 0.1145\n",
      "Epoch 624/1000 | Loss: 0.1143\n",
      "Epoch 625/1000 | Loss: 0.1142\n",
      "Epoch 626/1000 | Loss: 0.1141\n",
      "Epoch 627/1000 | Loss: 0.1140\n",
      "Epoch 628/1000 | Loss: 0.1139\n",
      "Epoch 629/1000 | Loss: 0.1138\n",
      "Epoch 630/1000 | Loss: 0.1137\n",
      "Epoch 631/1000 | Loss: 0.1135\n",
      "Epoch 632/1000 | Loss: 0.1134\n",
      "Epoch 633/1000 | Loss: 0.1133\n",
      "Epoch 634/1000 | Loss: 0.1132\n",
      "Epoch 635/1000 | Loss: 0.1131\n",
      "Epoch 636/1000 | Loss: 0.1130\n",
      "Epoch 637/1000 | Loss: 0.1129\n",
      "Epoch 638/1000 | Loss: 0.1127\n",
      "Epoch 639/1000 | Loss: 0.1126\n",
      "Epoch 640/1000 | Loss: 0.1125\n",
      "Epoch 641/1000 | Loss: 0.1124\n",
      "Epoch 642/1000 | Loss: 0.1123\n",
      "Epoch 643/1000 | Loss: 0.1122\n",
      "Epoch 644/1000 | Loss: 0.1121\n",
      "Epoch 645/1000 | Loss: 0.1120\n",
      "Epoch 646/1000 | Loss: 0.1119\n",
      "Epoch 647/1000 | Loss: 0.1117\n",
      "Epoch 648/1000 | Loss: 0.1116\n",
      "Epoch 649/1000 | Loss: 0.1115\n",
      "Epoch 650/1000 | Loss: 0.1114\n",
      "Epoch 651/1000 | Loss: 0.1113\n",
      "Epoch 652/1000 | Loss: 0.1112\n",
      "Epoch 653/1000 | Loss: 0.1111\n",
      "Epoch 654/1000 | Loss: 0.1110\n",
      "Epoch 655/1000 | Loss: 0.1109\n",
      "Epoch 656/1000 | Loss: 0.1108\n",
      "Epoch 657/1000 | Loss: 0.1107\n",
      "Epoch 658/1000 | Loss: 0.1106\n",
      "Epoch 659/1000 | Loss: 0.1105\n",
      "Epoch 660/1000 | Loss: 0.1103\n",
      "Epoch 661/1000 | Loss: 0.1102\n",
      "Epoch 662/1000 | Loss: 0.1101\n",
      "Epoch 663/1000 | Loss: 0.1100\n",
      "Epoch 664/1000 | Loss: 0.1099\n",
      "Epoch 665/1000 | Loss: 0.1098\n",
      "Epoch 666/1000 | Loss: 0.1097\n",
      "Epoch 667/1000 | Loss: 0.1096\n",
      "Epoch 668/1000 | Loss: 0.1095\n",
      "Epoch 669/1000 | Loss: 0.1094\n",
      "Epoch 670/1000 | Loss: 0.1093\n",
      "Epoch 671/1000 | Loss: 0.1092\n",
      "Epoch 672/1000 | Loss: 0.1091\n",
      "Epoch 673/1000 | Loss: 0.1090\n",
      "Epoch 674/1000 | Loss: 0.1089\n",
      "Epoch 675/1000 | Loss: 0.1088\n",
      "Epoch 676/1000 | Loss: 0.1087\n",
      "Epoch 677/1000 | Loss: 0.1086\n",
      "Epoch 678/1000 | Loss: 0.1085\n",
      "Epoch 679/1000 | Loss: 0.1084\n",
      "Epoch 680/1000 | Loss: 0.1083\n",
      "Epoch 681/1000 | Loss: 0.1082\n",
      "Epoch 682/1000 | Loss: 0.1081\n",
      "Epoch 683/1000 | Loss: 0.1080\n",
      "Epoch 684/1000 | Loss: 0.1079\n",
      "Epoch 685/1000 | Loss: 0.1078\n",
      "Epoch 686/1000 | Loss: 0.1077\n",
      "Epoch 687/1000 | Loss: 0.1076\n",
      "Epoch 688/1000 | Loss: 0.1075\n",
      "Epoch 689/1000 | Loss: 0.1074\n",
      "Epoch 690/1000 | Loss: 0.1073\n",
      "Epoch 691/1000 | Loss: 0.1072\n",
      "Epoch 692/1000 | Loss: 0.1071\n",
      "Epoch 693/1000 | Loss: 0.1070\n",
      "Epoch 694/1000 | Loss: 0.1069\n",
      "Epoch 695/1000 | Loss: 0.1068\n",
      "Epoch 696/1000 | Loss: 0.1067\n",
      "Epoch 697/1000 | Loss: 0.1066\n",
      "Epoch 698/1000 | Loss: 0.1065\n",
      "Epoch 699/1000 | Loss: 0.1064\n",
      "Epoch 700/1000 | Loss: 0.1063\n",
      "Epoch 701/1000 | Loss: 0.1062\n",
      "Epoch 702/1000 | Loss: 0.1061\n",
      "Epoch 703/1000 | Loss: 0.1060\n",
      "Epoch 704/1000 | Loss: 0.1059\n",
      "Epoch 705/1000 | Loss: 0.1058\n",
      "Epoch 706/1000 | Loss: 0.1057\n",
      "Epoch 707/1000 | Loss: 0.1056\n",
      "Epoch 708/1000 | Loss: 0.1055\n",
      "Epoch 709/1000 | Loss: 0.1055\n",
      "Epoch 710/1000 | Loss: 0.1054\n",
      "Epoch 711/1000 | Loss: 0.1053\n",
      "Epoch 712/1000 | Loss: 0.1052\n",
      "Epoch 713/1000 | Loss: 0.1051\n",
      "Epoch 714/1000 | Loss: 0.1050\n",
      "Epoch 715/1000 | Loss: 0.1049\n",
      "Epoch 716/1000 | Loss: 0.1048\n",
      "Epoch 717/1000 | Loss: 0.1047\n",
      "Epoch 718/1000 | Loss: 0.1046\n",
      "Epoch 719/1000 | Loss: 0.1045\n",
      "Epoch 720/1000 | Loss: 0.1044\n",
      "Epoch 721/1000 | Loss: 0.1043\n",
      "Epoch 722/1000 | Loss: 0.1043\n",
      "Epoch 723/1000 | Loss: 0.1042\n",
      "Epoch 724/1000 | Loss: 0.1041\n",
      "Epoch 725/1000 | Loss: 0.1040\n",
      "Epoch 726/1000 | Loss: 0.1039\n",
      "Epoch 727/1000 | Loss: 0.1038\n",
      "Epoch 728/1000 | Loss: 0.1037\n",
      "Epoch 729/1000 | Loss: 0.1036\n",
      "Epoch 730/1000 | Loss: 0.1035\n",
      "Epoch 731/1000 | Loss: 0.1034\n",
      "Epoch 732/1000 | Loss: 0.1033\n",
      "Epoch 733/1000 | Loss: 0.1033\n",
      "Epoch 734/1000 | Loss: 0.1032\n",
      "Epoch 735/1000 | Loss: 0.1031\n",
      "Epoch 736/1000 | Loss: 0.1030\n",
      "Epoch 737/1000 | Loss: 0.1029\n",
      "Epoch 738/1000 | Loss: 0.1028\n",
      "Epoch 739/1000 | Loss: 0.1027\n",
      "Epoch 740/1000 | Loss: 0.1026\n",
      "Epoch 741/1000 | Loss: 0.1026\n",
      "Epoch 742/1000 | Loss: 0.1025\n",
      "Epoch 743/1000 | Loss: 0.1024\n",
      "Epoch 744/1000 | Loss: 0.1023\n",
      "Epoch 745/1000 | Loss: 0.1022\n",
      "Epoch 746/1000 | Loss: 0.1021\n",
      "Epoch 747/1000 | Loss: 0.1020\n",
      "Epoch 748/1000 | Loss: 0.1019\n",
      "Epoch 749/1000 | Loss: 0.1019\n",
      "Epoch 750/1000 | Loss: 0.1018\n",
      "Epoch 751/1000 | Loss: 0.1017\n",
      "Epoch 752/1000 | Loss: 0.1016\n",
      "Epoch 753/1000 | Loss: 0.1015\n",
      "Epoch 754/1000 | Loss: 0.1014\n",
      "Epoch 755/1000 | Loss: 0.1014\n",
      "Epoch 756/1000 | Loss: 0.1013\n",
      "Epoch 757/1000 | Loss: 0.1012\n",
      "Epoch 758/1000 | Loss: 0.1011\n",
      "Epoch 759/1000 | Loss: 0.1010\n",
      "Epoch 760/1000 | Loss: 0.1009\n",
      "Epoch 761/1000 | Loss: 0.1008\n",
      "Epoch 762/1000 | Loss: 0.1008\n",
      "Epoch 763/1000 | Loss: 0.1007\n",
      "Epoch 764/1000 | Loss: 0.1006\n",
      "Epoch 765/1000 | Loss: 0.1005\n",
      "Epoch 766/1000 | Loss: 0.1004\n",
      "Epoch 767/1000 | Loss: 0.1004\n",
      "Epoch 768/1000 | Loss: 0.1003\n",
      "Epoch 769/1000 | Loss: 0.1002\n",
      "Epoch 770/1000 | Loss: 0.1001\n",
      "Epoch 771/1000 | Loss: 0.1000\n",
      "Epoch 772/1000 | Loss: 0.0999\n",
      "Epoch 773/1000 | Loss: 0.0999\n",
      "Epoch 774/1000 | Loss: 0.0998\n",
      "Epoch 775/1000 | Loss: 0.0997\n",
      "Epoch 776/1000 | Loss: 0.0996\n",
      "Epoch 777/1000 | Loss: 0.0995\n",
      "Epoch 778/1000 | Loss: 0.0995\n",
      "Epoch 779/1000 | Loss: 0.0994\n",
      "Epoch 780/1000 | Loss: 0.0993\n",
      "Epoch 781/1000 | Loss: 0.0992\n",
      "Epoch 782/1000 | Loss: 0.0991\n",
      "Epoch 783/1000 | Loss: 0.0991\n",
      "Epoch 784/1000 | Loss: 0.0990\n",
      "Epoch 785/1000 | Loss: 0.0989\n",
      "Epoch 786/1000 | Loss: 0.0988\n",
      "Epoch 787/1000 | Loss: 0.0987\n",
      "Epoch 788/1000 | Loss: 0.0987\n",
      "Epoch 789/1000 | Loss: 0.0986\n",
      "Epoch 790/1000 | Loss: 0.0985\n",
      "Epoch 791/1000 | Loss: 0.0984\n",
      "Epoch 792/1000 | Loss: 0.0983\n",
      "Epoch 793/1000 | Loss: 0.0983\n",
      "Epoch 794/1000 | Loss: 0.0982\n",
      "Epoch 795/1000 | Loss: 0.0981\n",
      "Epoch 796/1000 | Loss: 0.0980\n",
      "Epoch 797/1000 | Loss: 0.0980\n",
      "Epoch 798/1000 | Loss: 0.0979\n",
      "Epoch 799/1000 | Loss: 0.0978\n",
      "Epoch 800/1000 | Loss: 0.0977\n",
      "Epoch 801/1000 | Loss: 0.0976\n",
      "Epoch 802/1000 | Loss: 0.0976\n",
      "Epoch 803/1000 | Loss: 0.0975\n",
      "Epoch 804/1000 | Loss: 0.0974\n",
      "Epoch 805/1000 | Loss: 0.0973\n",
      "Epoch 806/1000 | Loss: 0.0973\n",
      "Epoch 807/1000 | Loss: 0.0972\n",
      "Epoch 808/1000 | Loss: 0.0971\n",
      "Epoch 809/1000 | Loss: 0.0970\n",
      "Epoch 810/1000 | Loss: 0.0970\n",
      "Epoch 811/1000 | Loss: 0.0969\n",
      "Epoch 812/1000 | Loss: 0.0968\n",
      "Epoch 813/1000 | Loss: 0.0967\n",
      "Epoch 814/1000 | Loss: 0.0967\n",
      "Epoch 815/1000 | Loss: 0.0966\n",
      "Epoch 816/1000 | Loss: 0.0965\n",
      "Epoch 817/1000 | Loss: 0.0964\n",
      "Epoch 818/1000 | Loss: 0.0964\n",
      "Epoch 819/1000 | Loss: 0.0963\n",
      "Epoch 820/1000 | Loss: 0.0962\n",
      "Epoch 821/1000 | Loss: 0.0961\n",
      "Epoch 822/1000 | Loss: 0.0961\n",
      "Epoch 823/1000 | Loss: 0.0960\n",
      "Epoch 824/1000 | Loss: 0.0959\n",
      "Epoch 825/1000 | Loss: 0.0959\n",
      "Epoch 826/1000 | Loss: 0.0958\n",
      "Epoch 827/1000 | Loss: 0.0957\n",
      "Epoch 828/1000 | Loss: 0.0956\n",
      "Epoch 829/1000 | Loss: 0.0956\n",
      "Epoch 830/1000 | Loss: 0.0955\n",
      "Epoch 831/1000 | Loss: 0.0954\n",
      "Epoch 832/1000 | Loss: 0.0953\n",
      "Epoch 833/1000 | Loss: 0.0953\n",
      "Epoch 834/1000 | Loss: 0.0952\n",
      "Epoch 835/1000 | Loss: 0.0951\n",
      "Epoch 836/1000 | Loss: 0.0951\n",
      "Epoch 837/1000 | Loss: 0.0950\n",
      "Epoch 838/1000 | Loss: 0.0949\n",
      "Epoch 839/1000 | Loss: 0.0948\n",
      "Epoch 840/1000 | Loss: 0.0948\n",
      "Epoch 841/1000 | Loss: 0.0947\n",
      "Epoch 842/1000 | Loss: 0.0946\n",
      "Epoch 843/1000 | Loss: 0.0946\n",
      "Epoch 844/1000 | Loss: 0.0945\n",
      "Epoch 845/1000 | Loss: 0.0944\n",
      "Epoch 846/1000 | Loss: 0.0944\n",
      "Epoch 847/1000 | Loss: 0.0943\n",
      "Epoch 848/1000 | Loss: 0.0942\n",
      "Epoch 849/1000 | Loss: 0.0941\n",
      "Epoch 850/1000 | Loss: 0.0941\n",
      "Epoch 851/1000 | Loss: 0.0940\n",
      "Epoch 852/1000 | Loss: 0.0939\n",
      "Epoch 853/1000 | Loss: 0.0939\n",
      "Epoch 854/1000 | Loss: 0.0938\n",
      "Epoch 855/1000 | Loss: 0.0937\n",
      "Epoch 856/1000 | Loss: 0.0937\n",
      "Epoch 857/1000 | Loss: 0.0936\n",
      "Epoch 858/1000 | Loss: 0.0935\n",
      "Epoch 859/1000 | Loss: 0.0935\n",
      "Epoch 860/1000 | Loss: 0.0934\n",
      "Epoch 861/1000 | Loss: 0.0933\n",
      "Epoch 862/1000 | Loss: 0.0933\n",
      "Epoch 863/1000 | Loss: 0.0932\n",
      "Epoch 864/1000 | Loss: 0.0931\n",
      "Epoch 865/1000 | Loss: 0.0931\n",
      "Epoch 866/1000 | Loss: 0.0930\n",
      "Epoch 867/1000 | Loss: 0.0929\n",
      "Epoch 868/1000 | Loss: 0.0929\n",
      "Epoch 869/1000 | Loss: 0.0928\n",
      "Epoch 870/1000 | Loss: 0.0927\n",
      "Epoch 871/1000 | Loss: 0.0927\n",
      "Epoch 872/1000 | Loss: 0.0926\n",
      "Epoch 873/1000 | Loss: 0.0925\n",
      "Epoch 874/1000 | Loss: 0.0925\n",
      "Epoch 875/1000 | Loss: 0.0924\n",
      "Epoch 876/1000 | Loss: 0.0923\n",
      "Epoch 877/1000 | Loss: 0.0923\n",
      "Epoch 878/1000 | Loss: 0.0922\n",
      "Epoch 879/1000 | Loss: 0.0921\n",
      "Epoch 880/1000 | Loss: 0.0921\n",
      "Epoch 881/1000 | Loss: 0.0920\n",
      "Epoch 882/1000 | Loss: 0.0919\n",
      "Epoch 883/1000 | Loss: 0.0919\n",
      "Epoch 884/1000 | Loss: 0.0918\n",
      "Epoch 885/1000 | Loss: 0.0917\n",
      "Epoch 886/1000 | Loss: 0.0917\n",
      "Epoch 887/1000 | Loss: 0.0916\n",
      "Epoch 888/1000 | Loss: 0.0915\n",
      "Epoch 889/1000 | Loss: 0.0915\n",
      "Epoch 890/1000 | Loss: 0.0914\n",
      "Epoch 891/1000 | Loss: 0.0913\n",
      "Epoch 892/1000 | Loss: 0.0913\n",
      "Epoch 893/1000 | Loss: 0.0912\n",
      "Epoch 894/1000 | Loss: 0.0912\n",
      "Epoch 895/1000 | Loss: 0.0911\n",
      "Epoch 896/1000 | Loss: 0.0910\n",
      "Epoch 897/1000 | Loss: 0.0910\n",
      "Epoch 898/1000 | Loss: 0.0909\n",
      "Epoch 899/1000 | Loss: 0.0908\n",
      "Epoch 900/1000 | Loss: 0.0908\n",
      "Epoch 901/1000 | Loss: 0.0907\n",
      "Epoch 902/1000 | Loss: 0.0906\n",
      "Epoch 903/1000 | Loss: 0.0906\n",
      "Epoch 904/1000 | Loss: 0.0905\n",
      "Epoch 905/1000 | Loss: 0.0905\n",
      "Epoch 906/1000 | Loss: 0.0904\n",
      "Epoch 907/1000 | Loss: 0.0903\n",
      "Epoch 908/1000 | Loss: 0.0903\n",
      "Epoch 909/1000 | Loss: 0.0902\n",
      "Epoch 910/1000 | Loss: 0.0902\n",
      "Epoch 911/1000 | Loss: 0.0901\n",
      "Epoch 912/1000 | Loss: 0.0900\n",
      "Epoch 913/1000 | Loss: 0.0900\n",
      "Epoch 914/1000 | Loss: 0.0899\n",
      "Epoch 915/1000 | Loss: 0.0898\n",
      "Epoch 916/1000 | Loss: 0.0898\n",
      "Epoch 917/1000 | Loss: 0.0897\n",
      "Epoch 918/1000 | Loss: 0.0897\n",
      "Epoch 919/1000 | Loss: 0.0896\n",
      "Epoch 920/1000 | Loss: 0.0895\n",
      "Epoch 921/1000 | Loss: 0.0895\n",
      "Epoch 922/1000 | Loss: 0.0894\n",
      "Epoch 923/1000 | Loss: 0.0894\n",
      "Epoch 924/1000 | Loss: 0.0893\n",
      "Epoch 925/1000 | Loss: 0.0892\n",
      "Epoch 926/1000 | Loss: 0.0892\n",
      "Epoch 927/1000 | Loss: 0.0891\n",
      "Epoch 928/1000 | Loss: 0.0891\n",
      "Epoch 929/1000 | Loss: 0.0890\n",
      "Epoch 930/1000 | Loss: 0.0889\n",
      "Epoch 931/1000 | Loss: 0.0889\n",
      "Epoch 932/1000 | Loss: 0.0888\n",
      "Epoch 933/1000 | Loss: 0.0888\n",
      "Epoch 934/1000 | Loss: 0.0887\n",
      "Epoch 935/1000 | Loss: 0.0886\n",
      "Epoch 936/1000 | Loss: 0.0886\n",
      "Epoch 937/1000 | Loss: 0.0885\n",
      "Epoch 938/1000 | Loss: 0.0885\n",
      "Epoch 939/1000 | Loss: 0.0884\n",
      "Epoch 940/1000 | Loss: 0.0883\n",
      "Epoch 941/1000 | Loss: 0.0883\n",
      "Epoch 942/1000 | Loss: 0.0882\n",
      "Epoch 943/1000 | Loss: 0.0882\n",
      "Epoch 944/1000 | Loss: 0.0881\n",
      "Epoch 945/1000 | Loss: 0.0881\n",
      "Epoch 946/1000 | Loss: 0.0880\n",
      "Epoch 947/1000 | Loss: 0.0879\n",
      "Epoch 948/1000 | Loss: 0.0879\n",
      "Epoch 949/1000 | Loss: 0.0878\n",
      "Epoch 950/1000 | Loss: 0.0878\n",
      "Epoch 951/1000 | Loss: 0.0877\n",
      "Epoch 952/1000 | Loss: 0.0877\n",
      "Epoch 953/1000 | Loss: 0.0876\n",
      "Epoch 954/1000 | Loss: 0.0875\n",
      "Epoch 955/1000 | Loss: 0.0875\n",
      "Epoch 956/1000 | Loss: 0.0874\n",
      "Epoch 957/1000 | Loss: 0.0874\n",
      "Epoch 958/1000 | Loss: 0.0873\n",
      "Epoch 959/1000 | Loss: 0.0873\n",
      "Epoch 960/1000 | Loss: 0.0872\n",
      "Epoch 961/1000 | Loss: 0.0871\n",
      "Epoch 962/1000 | Loss: 0.0871\n",
      "Epoch 963/1000 | Loss: 0.0870\n",
      "Epoch 964/1000 | Loss: 0.0870\n",
      "Epoch 965/1000 | Loss: 0.0869\n",
      "Epoch 966/1000 | Loss: 0.0869\n",
      "Epoch 967/1000 | Loss: 0.0868\n",
      "Epoch 968/1000 | Loss: 0.0867\n",
      "Epoch 969/1000 | Loss: 0.0867\n",
      "Epoch 970/1000 | Loss: 0.0866\n",
      "Epoch 971/1000 | Loss: 0.0866\n",
      "Epoch 972/1000 | Loss: 0.0865\n",
      "Epoch 973/1000 | Loss: 0.0865\n",
      "Epoch 974/1000 | Loss: 0.0864\n",
      "Epoch 975/1000 | Loss: 0.0864\n",
      "Epoch 976/1000 | Loss: 0.0863\n",
      "Epoch 977/1000 | Loss: 0.0862\n",
      "Epoch 978/1000 | Loss: 0.0862\n",
      "Epoch 979/1000 | Loss: 0.0861\n",
      "Epoch 980/1000 | Loss: 0.0861\n",
      "Epoch 981/1000 | Loss: 0.0860\n",
      "Epoch 982/1000 | Loss: 0.0860\n",
      "Epoch 983/1000 | Loss: 0.0859\n",
      "Epoch 984/1000 | Loss: 0.0859\n",
      "Epoch 985/1000 | Loss: 0.0858\n",
      "Epoch 986/1000 | Loss: 0.0858\n",
      "Epoch 987/1000 | Loss: 0.0857\n",
      "Epoch 988/1000 | Loss: 0.0856\n",
      "Epoch 989/1000 | Loss: 0.0856\n",
      "Epoch 990/1000 | Loss: 0.0855\n",
      "Epoch 991/1000 | Loss: 0.0855\n",
      "Epoch 992/1000 | Loss: 0.0854\n",
      "Epoch 993/1000 | Loss: 0.0854\n",
      "Epoch 994/1000 | Loss: 0.0853\n",
      "Epoch 995/1000 | Loss: 0.0853\n",
      "Epoch 996/1000 | Loss: 0.0852\n",
      "Epoch 997/1000 | Loss: 0.0852\n",
      "Epoch 998/1000 | Loss: 0.0851\n",
      "Epoch 999/1000 | Loss: 0.0851\n",
      "Epoch 1000/1000 | Loss: 0.0850\n"
     ]
    }
   ],
   "source": [
    "#main training loop\n",
    "n_epochs = 1000\n",
    "train_losses = np.zeros(n_epochs)\n",
    "test_losses = np.zeros(n_epochs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    #zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_scaled_tensor)\n",
    "    loss = criterion(outputs, y_train_scaled_tensor)\n",
    "\n",
    "    # append loss to a list of losses\n",
    "    train_losses[epoch] = loss.item()\n",
    "    outputs_test = model(X_test_scaled_tensor)\n",
    "    loss_test = criterion(outputs_test, y_test_scaled_tensor)\n",
    "    test_losses[epoch] = loss_test.item()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print the loss\n",
    "    print('Epoch {}/{} | Loss: {:.4f}'.format(epoch + 1, n_epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the values stored on train_losses and test_losses\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(test_losses, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9849246231155779\n",
      "Test accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "#get accuracy (this is just the average of the array where )\n",
    "with torch.no_grad():\n",
    "    p_train = model(X_train_scaled_tensor)\n",
    "    p_train = np.round(p_train)\n",
    "    train_acc = np.mean(y_train_scaled_tensor.numpy() == p_train.numpy())\n",
    "\n",
    "    p_test = model(X_test_scaled_tensor)\n",
    "    p_test = np.round(p_test)\n",
    "    test_acc = np.mean(y_test_scaled_tensor.numpy() == p_test.numpy())\n",
    "\n",
    "    print(\"Train accuracy: {}\".format(train_acc))\n",
    "    print(\"Test accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.5251, -0.4578, -0.4425, -0.1991, -0.1992, -0.2882, -0.3581, -0.3223,\n",
       "                       -0.3119,  0.5244, -0.2570,  0.0103, -0.1985, -0.3003, -0.1994,  0.0794,\n",
       "                       -0.0107, -0.0907,  0.2670,  0.3798, -0.4288, -0.5864, -0.4391, -0.3297,\n",
       "                       -0.2222, -0.2639, -0.1950, -0.4171, -0.4450, -0.1059]])),\n",
       "             ('0.bias', tensor([0.4221]))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save trained model with pytorch\n",
    "\n",
    "# first, let's look at the model as a dictionary:\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30471.91s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moore.csv  mymodel.pt  tutorial1.ipynb\ttutorial2.ipynb  tutorial3.ipynb\n"
     ]
    }
   ],
   "source": [
    "# save the dictionary of the model\n",
    "torch.save(model.state_dict(), 'mymodel.pt')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load the model, recreate the model then load the model dictionary\n",
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D,1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "#load mymodel.pt state dictionary onto model2\n",
    "model2.load_state_dict(torch.load('mymodel.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9849246231155779\n",
      "Test accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "#now let's test the accuracy for model2\n",
    "with torch.no_grad():\n",
    "    p_train = model2(X_train_scaled_tensor)\n",
    "    p_train = np.round(p_train)\n",
    "    train_acc = np.mean(y_train_scaled_tensor.numpy() == p_train.numpy())\n",
    "\n",
    "    p_test = model2(X_test_scaled_tensor)\n",
    "    p_test = np.round(p_test)\n",
    "    test_acc = np.mean(y_test_scaled_tensor.numpy() == p_test.numpy())\n",
    "\n",
    "    print(\"Train accuracy: {}\".format(train_acc))\n",
    "    print(\"Test accuracy: {}\".format(test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as seen, the result is the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
